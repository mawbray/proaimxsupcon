{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports \n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import jax\n",
    "import optax\n",
    "import jax.nn as jnn\n",
    "\n",
    "import jax.numpy as jnp\n",
    "import jax.random as jnr\n",
    "import jax.scipy as jsp\n",
    "from jax.example_libraries import optimizers\n",
    "import pyswarm\n",
    "\n",
    "import diffrax\n",
    "from diffrax import diffeqsolve, ODETerm, SaveAt, Tsit5, Dopri5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dynamics Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pure Mechanistic Model of Crystallisation of K2SO4\n",
    "\n",
    "\n",
    "From the following [source](https://pubs.acs.org/doi/10.1021/acs.iecr.3c00739)\n",
    "\n",
    "$$ \\frac{d}{dt}\\begin{bmatrix} \\mu_0 \\\\ \\mu_1 \\\\ \\mu_2 \\\\ \\mu_3 \\\\ C \\\\ T  \\end{bmatrix}  =  \\begin{bmatrix} r_{nuc}\\\\ r_1 (\\mu_0 + \\gamma \\mu_1 ) \\\\ 2r_1 (\\mu_1 +\\gamma \\mu_2) \\\\ 3r_1(\\mu_2 +\\gamma \\mu_3) \\\\ f(\\cdot)r_1(\\mu_2 + \\gamma \\mu_3) \\\\ f(T, Tc)  \\end{bmatrix}  $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' function to define the differential equations. Passed to diffeq solver '''\n",
    "\n",
    "def true_dynamics(t, state, params):\n",
    "  ka= 0.923714966\n",
    "  kb= -6754.878558\n",
    "  kc= 0.92229965554\n",
    "  kd= 1.341205945\n",
    "  kg= 48.07514464\n",
    "  k1= -4921.261419\n",
    "  k2= 1.871281405\n",
    "  a= 0.50523693\n",
    "  b= 7.271241375\n",
    "  alfa= 7.510905767\n",
    "  ro= 2.658  # g/cm³\n",
    "  V= 0.0005  # m³\n",
    "  ro_t= 997  # kg/m³\n",
    "  Cp_t= 4.117  # kJ/kg*K\n",
    "  UA= 0.2154  # kJ/min*K\n",
    "  Tc = 273+40\n",
    "\n",
    "  mu0, mu1, mu2, mu3, conc, temp = state\n",
    "\n",
    "\n",
    "  Ceq = -686.2686 + 3.579165 * jnp.array(temp) - 0.00292874 * jnp.array(temp) ** 2  # g/L  \n",
    "\n",
    "\n",
    "  S = jnp.array(conc) * 1e3 - Ceq  # g/L\n",
    "  B0 = ka * jnp.exp(kb / temp) * (S ** 2) ** (kc / 2) * ((mu3 ** 2) ** (kd / 2))  # /(cm³*min)\n",
    "  Ginf = kg * jnp.exp(k1 / temp) * (S ** 2) ** (k2 / 2)  # [G] = [Ginf] = cm/min\n",
    "\n",
    "\n",
    "  dmi0dt = B0\n",
    "  dmi1dt = Ginf * (a * mu0 + b * mu1 * 1e-4) * 1e4\n",
    "  dmi2dt = 2 * Ginf * (a * mu1 * 1e-4 + b * mu2 * 1e-8) * 1e8\n",
    "  dmi3dt = 3 * Ginf * (a * mu2 * 1e-8 + b * mu3 * 1e-12) * 1e12\n",
    "  dcdt = -0.5 * ro * alfa * Ginf * (a * mu2 * 1e-8 + b * mu3 * 1e-12)\n",
    "  dTdt = UA * (Tc - temp) / (V * ro_t * Cp_t)\n",
    "\n",
    "  dxdt = jnp.array([dmi0dt, dmi1dt, dmi2dt, dmi3dt, dcdt, dTdt])\n",
    "  return dxdt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Dataset\n",
    "\n",
    "Create samples based on true dynamics model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data(parameters, T0, TF, steps, step_size, x0):\n",
    "  T = jnp.linspace(T0, TF, steps + 1)\n",
    "  x_true = true_dynamics_rk4(parameters, T0, TF, steps, step_size, x0)\n",
    "  jax.debug.print(\"True State Shape = {}\".format(x_true.shape))\n",
    "\n",
    "  random_noise = np.random.multivariate_normal(np.array([0]*x_true.shape[1]), np.diag(np.ones(x_true.shape[1])), steps + 1)\n",
    "  x_obs = x_true * (1 + random_noise * 0.02)\n",
    "\n",
    "  print(\"Observed State Shape = {}\".format(x_obs.shape))\n",
    "\n",
    "  return x_true, x_obs, T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_samples(x_true, x_obs, T):\n",
    "  font = {'family' : 'serif',\n",
    "        'weight' : 'bold'}\n",
    "\n",
    "  plt.rc('font', **font)\n",
    "  label_tag = ['$\\mu_0$', '$\\mu_1$', '$\\mu_2$', '$\\mu_3$', 'C', 'T']\n",
    "  for i in range(x_obs.shape[1]):\n",
    "    plt.plot(T, (x_obs.T[i, :]- np.min(x_obs.T[i, :]))/(np.max(x_obs.T[i, :]) -np.min(x_obs.T[i, :])), '.', label = \"Obs \" + label_tag[i])\n",
    "    plt.plot(T, (x_true.T[i, :]- np.min(x_obs.T[i, :]))/(np.max(x_obs.T[i, :])- np.min(x_obs.T[i, :])), '-', label = \"True \" + label_tag[i])\n",
    "\n",
    "  plt.xlabel(\"Time\")\n",
    "  plt.ylabel(\"State\")\n",
    "  plt.title(\"Normalized state values over time\")\n",
    "  plt.legend()\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Model\n",
    "\n",
    "The model presented above is semi-empirical given it incorporates a quadratic function of T to describe temperture dependence in the solute equilibrium concentration\n",
    "\n",
    "\n",
    "$$ r_{nuc} = k_b \\exp\\left(\\frac{-E_{Ab}}{RT}\\right)ΔC^b\\mu_3^\\beta $$\n",
    "$$\\Delta C = C - C_s $$\n",
    "\n",
    "\n",
    "$$ C_s(T) =-686.27 +3.5795T - 2.9287 \\times 10^{-3}T^2  $$\n",
    "\n",
    "with $C_s(T)$ identified in preliminary experiments.\n",
    "\n",
    "\n",
    "Here, we will remove the quadratic expression from the model and try to learn the temperature dependence directly from reactor data as a neural network\n",
    "\n",
    "$$ C_{eq}(T) = f_{NN}(T, \\phi)$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# , [0.5], [225.0] , [1],[100.0]\n",
    "def input_scaler(inputs, scale_mean = jnp.array([155323e2*2, 2123753e2, 184700097, 2340672717.3,0.1605, 273+39.38]), scale_std = 0.5 ):\n",
    "  # assumes state space bounded of NN is bounded by tanh\n",
    "  return jnp.divide(inputs-scale_mean, jnp.divide(scale_mean,scale_std))\n",
    "\n",
    "def minmax_scalar(inputs, range=jnp.array([3.6641571e+02]), mini=jnp.array([2.9449063e+02]) ):    # 3.1358778e+08, 2.6019895e+09, 7.2683312e+09, 3.8483950e+10,2.8258935e-01,     1.5750447e+03, 2.2706311e+04, 1.8967081e+06, 2.4140062e+08, 1.3892503e-01,\n",
    "  return jnp.divide(inputs + mini, range)\n",
    "\n",
    "def output_scaler(inputs, scale_min = -1, scale_max = 1):\n",
    "  return jnp.multiply(jnp.divide(inputs + jnp.array([1]), jnp.array([2])), scale_max - scale_min) + scale_min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "''' Parameter initialization functions '''\n",
    "\n",
    "def init_layer(m, n, rng):\n",
    "  Wm_rng, bm_rng = jnr.split(rng, 2)\n",
    "  std = 1./jnp.sqrt(n)\n",
    "  W_mu = jnr.uniform(Wm_rng, (n, m), minval = -std, maxval = std)\n",
    "  b_mu = jnr.uniform(bm_rng, (n, 1), minval = -std, maxval = std)\n",
    "\n",
    "  params = {\n",
    "      'W' : W_mu,\n",
    "      'b' : b_mu }\n",
    "\n",
    "  return params\n",
    "\n",
    "def init_bnn_params(init_rng, sizes):\n",
    "  rngs = jnr.split(init_rng, len(sizes))\n",
    "  return {i:init_layer(m, n, k) for i, (k, m, n) in enumerate(zip(rngs, sizes[:-1], sizes[1:]))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Prediction functions '''\n",
    "\n",
    "def relu(x):\n",
    "  return jnp.maximum(0, x)\n",
    "\n",
    "\n",
    "def relu_6(x):\n",
    "  return jnp.divide(jnp.minimum(6, jnp.maximum(0, x)),6)\n",
    "\n",
    "\n",
    "def tanh(x):\n",
    "  return jnp.tanh(x)\n",
    "\n",
    "\n",
    "def predict(params, x):\n",
    "  activations = x.reshape(-1,)\n",
    "\n",
    "  activations = minmax_scalar(activations).reshape(-1,1)\n",
    "\n",
    "  #jax.debug.print('x = {x}', x=activations)\n",
    "\n",
    "\n",
    "\n",
    "  for i in range(len(params)-1):\n",
    "\n",
    "    W = params[i]['W']\n",
    "    b = params[i]['b']\n",
    "\n",
    "    activations = tanh(jnp.dot(W, activations) + b)\n",
    "\n",
    "\n",
    "  final_W = params[i+1]['W']\n",
    "  final_b = params[i+1]['b']\n",
    "\n",
    "\n",
    "\n",
    "  output = jnp.tanh(jnp.dot(final_W, activations) + final_b) + 1\n",
    "\n",
    "  #jax.debug.print('x = {x}', x=output.squeeze()/2)\n",
    "\n",
    "  return output.squeeze()/2\n",
    "\n",
    "batched_predict = jax.vmap(predict, in_axes = (None, 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hybrid Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "''' function to define the differential equation using NN output for u_x '''\n",
    "\n",
    "def hybrid_dynamics(t, state, params):\n",
    "  ka= 0.923714966\n",
    "  kb= -6754.878558\n",
    "  kc= 0.92229965554\n",
    "  kd= 1.341205945\n",
    "  kg= 48.07514464\n",
    "  k1= -4921.261419\n",
    "  k2= 1.871281405\n",
    "  a= 0.50523693\n",
    "  b= 7.271241375\n",
    "  alfa= 7.510905767\n",
    "  ro= 2.658  # g/cm³\n",
    "  V= 0.0005  # m³\n",
    "  ro_t= 997  # kg/m³\n",
    "  Cp_t= 4.117  # kJ/kg*K\n",
    "  UA= 0.2154  # kJ/min*K\n",
    "  Tc = 273+40\n",
    "\n",
    "  mu0, mu1, mu2, mu3, conc, temp = state\n",
    "\n",
    "\n",
    "  Ceq =  180 * predict(params, jnp.copy(temp)) # -686.2686 + 3.579165 * jnp.array(temp) - 0.00292874 * jnp.array(temp) ** 2  # g/L\n",
    "  S = jnp.array(conc) * 1e3 - Ceq  # g/L\n",
    "\n",
    "  B0 = ka * jnp.exp(kb / temp) * (S ** 2) ** (kc / 2) * ((mu3 ** 2) ** (kd / 2))  # /(cm³*min)\n",
    "  Ginf = kg * jnp.exp(k1 / temp) * (S ** 2) ** (k2 / 2)  # [G] = [Ginf] = cm/min\n",
    "\n",
    "\n",
    "  dmi0dt = B0\n",
    "  dmi1dt = Ginf * (a * mu0 + b * mu1 * 1e-4) * 1e4\n",
    "  dmi2dt = 2 * Ginf * (a * mu1 * 1e-4 + b * mu2 * 1e-8) * 1e8\n",
    "  dmi3dt = 3 * Ginf * (a * mu2 * 1e-8 + b * mu3 * 1e-12) * 1e12\n",
    "  dcdt = -0.5 * ro * alfa * Ginf * (a * mu2 * 1e-8 + b * mu3 * 1e-12)\n",
    "  dTdt = UA * (Tc - temp) / (V * ro_t * Cp_t)\n",
    "\n",
    "  dxdt = jnp.hstack([dmi0dt.reshape(1,1), dmi1dt.reshape(1,1), dmi2dt.reshape(1,1), dmi3dt.reshape(1,1), dcdt.reshape(1,1), jnp.array(dTdt).reshape(1,1)])\n",
    "\n",
    "  return dxdt.squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Integration Schemes for Hybrid Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hybrid_dynamics_rk4(params, t0, tf, steps, dt0, x0):\n",
    "  term = ODETerm(hybrid_dynamics)\n",
    "  solver = Tsit5()\n",
    "  saveat = SaveAt(ts=jnp.linspace(t0, tf, steps + 1))\n",
    "\n",
    "  return diffeqsolve(term, solver, t0, tf, dt0, x0, args = params, saveat = saveat).ys\n",
    "\n",
    "def hybrid_dynamics_rk4_1s(params, t0, tf, steps, dt0, x0):\n",
    "  term = ODETerm(hybrid_dynamics)\n",
    "  solver =diffrax.Kvaerno5()\n",
    "  stepsize_controller = diffrax.PIDController(rtol=1e-1, atol=1e-1)\n",
    "  saveat = SaveAt(ts=jnp.linspace(t0, tf, steps + 1))\n",
    "  #print(saveat)\n",
    "  #adjoint_controller = diffrax.PIDController(rtol=1e-1, atol=1e-2, norm = diffrax.adjoint_rms_seminorm)\n",
    "  adjoint = diffrax.RecursiveCheckpointAdjoint()\n",
    "  try:\n",
    "    return diffeqsolve(term, solver, t0, tf, dt0, x0, args = params,stepsize_controller=stepsize_controller, adjoint=adjoint, saveat = saveat, max_steps=100000).ys[-1, :]\n",
    "  except:\n",
    "    return x0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Root Mean Squared Error Loss '''\n",
    "\n",
    "def rmse(x, y):\n",
    "  return jnp.sqrt(jnp.mean(jnp.power(x - y, 2)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Negative Log Likelihood function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialise_Qmat(x_obs):\n",
    "  Qmat = jnp.diag(jnp.power(jnp.std(x_obs.T, axis = 1), 2))\n",
    "  Qmat = jnp.linalg.inv(Qmat)\n",
    "\n",
    "  return Qmat\n",
    "\n",
    "def NLL_hybrid(bnn_params, rng, t0, tf, steps, step_size, x0, x_obs, Qmat):\n",
    "  dt = tf/steps\n",
    "  xt = x0\n",
    "  nll = jnp.array([0])\n",
    "\n",
    "  # One step at a time\n",
    "  for i in range(steps):\n",
    "    xt = hybrid_dynamics_rk4_1s(bnn_params, t0, dt, 1, step_size, xt)\n",
    "    xerror = x_obs[i+1].reshape(1, -1) - xt.reshape(1, -1)\n",
    "    nll += jnp.matmul(jnp.matmul(xerror, Qmat), xerror.T)\n",
    "\n",
    "  #jax.debug.print(\"Total NLL Loss = {x}\", x = nll)\n",
    "  #jax.debug.print(\"Total KLD Loss = {x}\", x = kld_lossval)\n",
    "  return nll.squeeze()\n",
    "\n",
    "def NLL_hybrid_integrator(bnn_params, rng, t0, tf, steps, step_size, x0, x_obs, Qmat):\n",
    "  dt = tf/steps\n",
    "  xt = x0\n",
    "\n",
    "  nll = jnp.array([0])\n",
    "  x_history = xt.reshape(1, -1)\n",
    "\n",
    "  for i in range(steps):\n",
    "    xt = hybrid_dynamics_rk4_1s(bnn_params, t0, dt, 1, step_size, xt)\n",
    "    x_history = jnp.append(x_history, xt.reshape(1, -1), axis = 0)\n",
    "\n",
    "  return x_history\n",
    "\n",
    "\n",
    "def NLL_true(parameters, t0, tf, steps, step_size, x0, x_obs, Qmat):\n",
    "  xt = true_dynamics_rk4(parameters, t0, tf, steps, step_size, x0)\n",
    "  xerror = x_obs - xt\n",
    "  nll = jnp.array([0])\n",
    "\n",
    "  for i in range(xerror.shape[0]):\n",
    "    jax.debug.print(\"State error at index {x[1]} {x[0]} \", x = [xerror[i], i])\n",
    "    nll += jnp.matmul(jnp.matmul(xerror[i], Qmat), xerror[i].T)\n",
    "    jax.debug.print(\"NLL contribution at index {x[1]} is {x[0]}\", x = [jnp.matmul(jnp.matmul(xerror[i], Qmat), xerror[i].T), i])\n",
    "\n",
    "  return nll.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NLL_true_parameters(parameters, t0, tf, steps, step_size, x0, x_obs, Qmat ):\n",
    "  return NLL_true(parameters, t0, tf, steps, step_size, x0, x_obs, Qmat)\n",
    "\n",
    "def NLL_hybrid_parameters(bnn_params, rng, t0, tf, steps, step_size, x0, x_obs, Qmat):\n",
    "  return NLL_hybrid(bnn_params, rng, t0, tf, steps, step_size, x0, x_obs, Qmat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_f = jax.value_and_grad(NLL_hybrid_parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T0 = 0\n",
    "TF = 70\n",
    "paramzz = ()\n",
    "steps = 20\n",
    "step_size = 0.05\n",
    "x0 = jnp.array([1553.23, 21237.53, 1847000.97, 234067271.73,0.1605, 273+39.38]) # Exp 4 from the paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_true, x_obs, T = create_data(paramzz, T0, TF, steps, step_size, x0)\n",
    "plot_samples(x_true, x_obs, T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Qmat = initialise_Qmat(x_obs)\n",
    "Qmat = Qmat.at[0,0].set(1e-8)\n",
    "Qmat = Qmat.at[1,1].set(1e-8)\n",
    "Qmat = Qmat.at[2,2].set(1e-8)\n",
    "Qmat = Qmat.at[3,3].set(1e-8)\n",
    "Qmat = Qmat.at[4,4].set(1e-3)\n",
    "Qmat = Qmat.at[5,5].set(1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Reference loss for the true data generating model\n",
    "\n",
    " cost = NLL_true_parameters(paramzz, T0, TF, steps, step_size, x0, x_obs, Qmat)\n",
    " print('sum of stage costs', cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_TARGETS = 1\n",
    "N_INPUTS = 1\n",
    "SIZES = [N_INPUTS, 10, 2, N_TARGETS]\n",
    "GAMMA = 0.00008\n",
    "rng = jnr.PRNGKey(10)\n",
    "NUM_EPOCHS = 100\n",
    "\n",
    "\n",
    "# Initialize bnn model\n",
    "bnn_params = init_bnn_params(rng, SIZES)\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from pyswarm import pso  # If not installed, run `pip install pyswarm`\n",
    "import jax.random as jnr\n",
    "import jax.numpy as jnp\n",
    "\n",
    "# Define the wrapper function to initialize the BNN parameters\n",
    "def init_layer(m, n, rng):\n",
    "    Wm_rng, bm_rng = jnr.split(rng, 2)\n",
    "    std = 1.0 / jnp.sqrt(n)\n",
    "    W_mu = jnr.uniform(Wm_rng, (n, m), minval=-std, maxval=std)\n",
    "    b_mu = jnr.uniform(bm_rng, (n, 1), minval=-std, maxval=std)\n",
    "\n",
    "    params = {'W': W_mu, 'b': b_mu}\n",
    "    return params\n",
    "\n",
    "def init_bnn_params(init_rng, sizes):\n",
    "    rngs = jnr.split(init_rng, len(sizes))\n",
    "    return {i: init_layer(m, n, k) for i, (k, m, n) in enumerate(zip(rngs, sizes[:-1], sizes[1:]))}\n",
    "\n",
    "# Flatten the bnn_params dictionary for use in PSO\n",
    "def flatten_bnn_params(bnn_params):\n",
    "    flat_params = []\n",
    "    for layer_params in bnn_params.values():\n",
    "        flat_params.append(layer_params['W'].ravel())\n",
    "        flat_params.append(layer_params['b'].ravel())\n",
    "    return np.concatenate(flat_params)\n",
    "\n",
    "# Reconstruct the bnn_params dictionary from flat_params\n",
    "def unflatten_bnn_params(flat_params, sizes):\n",
    "    bnn_params = {}\n",
    "    index = 0\n",
    "    for i, (m, n) in enumerate(zip(sizes[:-1], sizes[1:])):\n",
    "        W_size = n * m\n",
    "        b_size = n\n",
    "\n",
    "        W = flat_params[index:index + W_size].reshape((n, m))\n",
    "        index += W_size\n",
    "        b = flat_params[index:index + b_size].reshape((n, 1))\n",
    "        index += b_size\n",
    "\n",
    "        bnn_params[i] = {'W': W, 'b': b}\n",
    "    return bnn_params\n",
    "\n",
    "# Define the target function wrapper for PSO\n",
    "def NLL_hybrid_flattened_params(flat_params, rng, t0, tf, steps, step_size, x0, x_obs, Qmat, sizes):\n",
    "    bnn_params = unflatten_bnn_params(flat_params, sizes)\n",
    "    return NLL_hybrid_parameters(bnn_params, rng, t0, tf, steps, step_size, x0, x_obs, Qmat)\n",
    "\n",
    "# Define the Particle Swarm Optimization (PSO) function\n",
    "def particle_swarm_optimize_NLL(rng, t0, tf, steps, step_size, x0, x_obs, Qmat, sizes, num_particles=50, max_iter=100):\n",
    "    # Flatten initial parameters to determine dimensionality\n",
    "    example_bnn_params = init_bnn_params(rng, sizes)\n",
    "    flat_example_params = flatten_bnn_params(example_bnn_params)\n",
    "    dim = flat_example_params.shape[0]\n",
    "\n",
    "    # Define the bounds for each parameter; adjust as needed\n",
    "    lb = -1.0 * np.ones(dim)  # lower bounds for each parameter\n",
    "    ub = 1.0 * np.ones(dim)   # upper bounds for each parameter\n",
    "\n",
    "    # Run PSO to minimize the NLL function\n",
    "    best_params, best_cost = pso(\n",
    "        NLL_hybrid_flattened_params,\n",
    "        lb,\n",
    "        ub,\n",
    "        args=(rng, t0, tf, steps, step_size, x0, x_obs, Qmat, sizes),\n",
    "        swarmsize=num_particles,\n",
    "        maxiter=max_iter\n",
    "    )\n",
    "\n",
    "    # Reconstruct the best bnn_params dictionary from the best parameters\n",
    "    bnn_params_best = unflatten_bnn_params(best_params, sizes)\n",
    "    return bnn_params_best, best_cost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bnns_params_best, best_cost = particle_swarm_optimize_NLL(rng, T0, TF, steps, step_size, x0, x_obs, Qmat, SIZES, num_particles=5, max_iter=20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lossvals = []\n",
    "bnn_params = bnns_params_best\n",
    "\n",
    "# ---- ADAM optimizer (works pretty well here) ---- #\n",
    "\n",
    "linear_decay_scheduler = optax.linear_schedule(init_value=0.001, end_value=0.0000009,\n",
    "                                               transition_steps=NUM_EPOCHS,\n",
    "                                               transition_begin=int(NUM_EPOCHS*0.25))\n",
    "\n",
    "optimizer = optax.adam(linear_decay_scheduler)\n",
    "opt_state = optimizer.init(bnn_params)\n",
    "\n",
    "def train_step(opt_state, bnn_params):\n",
    "  lossval, grads = grad_f(bnn_params, rng, T0, TF, steps, step_size, x0, x_obs, Qmat)\n",
    "  updates, opt_state = optimizer.update(grads, opt_state, bnn_params)\n",
    "  bnn_params = optax.apply_updates(bnn_params, updates)\n",
    "  return lossval, opt_state, bnn_params\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "  print(epoch)\n",
    "  lossval, opt_state, bnn_params = train_step(opt_state, bnn_params)\n",
    "  lossvals.append(lossval)\n",
    "  if (epoch + 1) % 5 == 0:\n",
    "     print(f\"At epoch {epoch+1} loss = {lossval}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step LR by 0.75 every 100 steps\n",
    "font = {'family': 'serif', 'weight': 'bold'}\n",
    "plt.rc('font', **font)\n",
    "plt.plot(range(NUM_EPOCHS), jnp.array(lossvals), linestyle='--', color='blue', marker='o', markersize=1)\n",
    "plt.xlabel('Epochs', fontsize=12)\n",
    "plt.ylabel('NLL Loss', fontsize=12)\n",
    "plt.title('Training Loss over Epochs')\n",
    "plt.grid(True)\n",
    "plt.xticks(fontsize=10)\n",
    "plt.yticks(fontsize=10)\n",
    "plt.ylim(min(lossvals),2.5e9)\n",
    "plt.xlim(0,200)\n",
    "plt.savefig('adjoint_trainer.svg')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = jnr.PRNGKey(11)\n",
    "histories = []\n",
    "for n in range(1):\n",
    "  new_rng, rng = jnr.split(rng)\n",
    "  x_history = NLL_hybrid_integrator(bnn_params, rng, T0, TF, steps, step_size, x0, x_obs, Qmat)\n",
    "  histories.append(x_history)\n",
    "\n",
    "histories = jnp.array(histories)\n",
    "history_mean = jnp.mean(histories, axis=0)\n",
    "history_std = jnp.std(histories, axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "font = {'family': 'serif', 'weight': 'bold'}\n",
    "plt.rc('font', **font)\n",
    "for i in range(x_obs.shape[1]):\n",
    "  plt.figure(figsize = (10,8))\n",
    "  plt.plot(T, x_obs.T[i, :], '.', label = label_tag[i])\n",
    "  plt.plot(T, history_mean.T[i, :], '-', label = 'Hybrid ' + label_tag[i])\n",
    "  plt.plot(T, x_true.T[i, :], '-', label = 'Mechanistic ' + label_tag[i])\n",
    "  plt.legend()\n",
    "  plt.xlabel(\"Time\")\n",
    "  plt.ylabel(label_tag[i])\n",
    "  plt.xlim(0,70)\n",
    "  plt.savefig(label_tag[i] + '.svg')\n",
    "  plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
